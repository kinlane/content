---
title: On Device Machine Learning API Stack
date: 2017-05-17 18:59:00 Z
---

I was reading about [Google's TensorFlowLite in Techcrunch](https://techcrunch.com/2017/05/17/googles-tensorflow-lite-brings-machine-learning-to-android-devices/?ncid=rss), and their mention of [Facebook's Caffe2Go](https://code.facebook.com/posts/196146247499076/delivering-real-time-ai-in-the-palm-of-your-hand/), and I was reminded of a conversation I was having with the Oxford Dictionaries API team a couple months ago. 

The OED and other dictionary and language content API teams wanted to learn more about on-device API deployment. I have asked when we will have containers natively on our routers a while ago, but I'd also like to add to that request, and ask when will we have a stack of containers on device where we can deploy API resources that can be used by applications, and augment the existing on-device hardware and OS APIs.

